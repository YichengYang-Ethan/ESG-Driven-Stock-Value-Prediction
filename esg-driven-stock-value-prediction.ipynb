{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45734ba7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-08-16T15:31:53.119407",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": "# ESG-Driven Stock Value Prediction\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# ============ 1. Feature Engineering ============\ndef create_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Creates new features from the raw dataframe.\"\"\"\n    # Composite ESG score\n    df[\"composite_esg\"] = (df[\"env\"] + df[\"social\"] + df[\"governance\"]) / 3.0\n    \n    # Sort by ticker and date to prepare for rolling calculations\n    df = df.sort_values([\"ticker\", \"date\"])\n    \n    # 5-period price momentum (percentage change)\n    df[\"momentum_5d\"] = df.groupby(\"ticker\")[\"price\"].pct_change(5)\n    \n    # 10-period rolling mean price\n    df[\"rolling_mean_10\"] = df.groupby(\"ticker\")[\"price\"].transform(lambda x: x.rolling(10).mean())\n    \n    # Drop rows with NaN values resulting from rolling calculations\n    df = df.dropna(subset=[\"momentum_5d\", \"rolling_mean_10\", \"composite_esg\"])\n    return df\n\n# ============ 2. Walk-Forward Backtesting ============\ndef _make_class_labels(y_true_block):\n    \"\"\"\n    Helper function to create classification labels for a block of price data.\n    Prices above the median for the block are labeled 1, others are 0.\n    \"\"\"\n    median_price = np.median(y_true_block)\n    return (y_true_block > median_price).astype(int)\n\ndef walk_forward_backtest(df, features, rf_model, log_model, n_splits=5, target_col=\"price\"):\n    \"\"\"\n    Performs a walk-forward backtest and returns detailed performance metrics.\n    StandardScaler is fit ONLY on the training fold to prevent data leakage.\n    \"\"\"\n    df = df.sort_values(\"date\")\n    dates = df[\"date\"].unique()\n    split_size = len(dates) // n_splits\n\n    # Lists to store results from each fold\n    rf_rmse_list, log_rmse_list = [], []\n    rf_acc_list,  log_acc_list  = [], []\n    fold_labels = []\n\n    print(f\"Starting walk-forward backtest with {n_splits} splits...\")\n    for i in range(n_splits):\n        # Define the date ranges for training and testing sets\n        train_dates = dates[: (i + 1) * split_size]\n        test_dates  = dates[(i + 1) * split_size : (i + 2) * split_size]\n        \n        if len(test_dates) == 0:\n            continue\n\n        fold_label = f\"{pd.to_datetime(test_dates[0]).strftime('%Y-%m')} to {pd.to_datetime(test_dates[-1]).strftime('%Y-%m')}\"\n        fold_labels.append(f\"Fold {i+1}\\n({fold_label})\")\n        print(f\"  - Fold {i+1}/{n_splits}: Training up to {pd.to_datetime(train_dates[-1]).strftime('%Y-%m')}, Testing on {fold_label}\")\n\n        # Split data into training and testing sets\n        train = df[df[\"date\"].isin(train_dates)]\n        test  = df[df[\"date\"].isin(test_dates)]\n\n        X_train, y_train = train[features].copy(), train[target_col].values\n        X_test,  y_test  = test[features].copy(),  test[target_col].values\n\n        # Fit scaler on training data ONLY, then transform both train and test\n        scaler = StandardScaler()\n        X_train[features] = scaler.fit_transform(X_train)\n        X_test[features] = scaler.transform(X_test)\n\n        # ---- Random Forest: Regress on price, then convert to classification ----\n        rf_model.fit(X_train, y_train)\n        rf_preds_reg = rf_model.predict(X_test)\n        rf_rmse_list.append(mean_squared_error(y_test, rf_preds_reg, squared=False))\n\n        y_test_cls = _make_class_labels(y_test)\n        rf_preds_cls = (rf_preds_reg > np.median(rf_preds_reg)).astype(int)\n        rf_acc_list.append(accuracy_score(y_test_cls, rf_preds_cls))\n\n        # ---- Logistic Regression: Baseline classification model ----\n        y_train_cls = _make_class_labels(y_train)\n        log_model.fit(X_train, y_train_cls)\n        log_preds_cls = log_model.predict(X_test)\n        log_acc_list.append(accuracy_score(y_test_cls, log_preds_cls))\n        \n        log_probs = log_model.predict_proba(X_test)[:, 1]\n        log_rmse_list.append(mean_squared_error(y_test, log_probs * np.mean(y_test), squared=False))\n\n    results = {\n        \"rf_rmse\": np.mean(rf_rmse_list), \"log_rmse\": np.mean(log_rmse_list),\n        \"rf_acc\": np.mean(rf_acc_list), \"log_acc\": np.mean(log_acc_list),\n        \"lift\": (np.mean(rf_acc_list) - np.mean(log_acc_list)) / np.mean(log_acc_list),\n        \"rf_acc_folds\": rf_acc_list, \"log_acc_folds\": log_acc_list,\n        \"fold_labels\": fold_labels\n    }\n    return results\n\n# ============ 3. Load Existing Data from CSV ============\nprint(\"Loading existing data from CSV file...\")\ntry:\n    file_path = 'my_esg_stock_data.csv' \n    df = pd.read_csv(file_path, parse_dates=['date'])\n    print(f\"Data loaded successfully from {file_path}. Shape: {df.shape}\")\nexcept FileNotFoundError:\n    print(f\"Error: The file '{file_path}' was not found. Please check the file path and try again.\")\n    exit() \n\n\n# ============ 4. Full Pipeline ============\nprint(\"Handling missing values...\")\ndf = df.fillna(df.median(numeric_only=True))\n\nprint(\"Creating features...\")\ndf = create_features(df)\nprint(f\"Shape after feature engineering: {df.shape}\")\n\n# Feature columns (scaling is now done inside the walk-forward loop to prevent data leakage)\nfeature_cols = [c for c in df.columns if c not in [\"date\", \"ticker\", \"price\"]]\n\n# Model Definition\nrf_model  = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, min_samples_leaf=10)\nlog_model = LogisticRegression(max_iter=1000, random_state=42)\n\n# Run Walk-Forward Backtest\nresults = walk_forward_backtest(df, feature_cols, rf_model, log_model, n_splits=5)\n\n# Print Final Results\nprint(\"\\n\" + \"=\"*25)\nprint(\"=== Backtest Results ===\")\nprint(\"=\"*25)\nprint(f\"Random Forest Avg. RMSE: {results['rf_rmse']:.4f}\")\nprint(f\"Logistic Reg. Avg. RMSE: {results['log_rmse']:.4f}\")\nprint(\"-\" * 25)\nprint(f\"Random Forest Avg. Accuracy: {results['rf_acc']:.4f}\")\nprint(f\"Logistic Reg.  Avg. Accuracy: {results['log_acc']:.4f}\")\nprint(\"-\" * 25)\nprint(f\"Relative Lift in Classification Accuracy: {results['lift']*100:.2f}%\")\nprint(\"=\"*25 + \"\\n\")\n\n\n# ============ 5. Visualization ============\nif results and results['fold_labels']:\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(12, 7))\n\n    bar_width = 0.35\n    index = np.arange(len(results['fold_labels']))\n\n    bar1 = plt.bar(index - bar_width/2, results['rf_acc_folds'], bar_width, label='Random Forest', color='royalblue', alpha=0.9)\n    bar2 = plt.bar(index + bar_width/2, results['log_acc_folds'], bar_width, label='Logistic Regression', color='darkorange', alpha=0.9)\n\n    for bar in bar1:\n        yval = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.3f}', va='bottom', ha='center', fontsize=9)\n    for bar in bar2:\n        yval = bar.get_height()\n        plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.3f}', va='bottom', ha='center', fontsize=9)\n\n    plt.xlabel(\"Backtest Fold\", fontsize=12)\n    plt.ylabel(\"Classification Accuracy\", fontsize=12)\n    plt.title(\"Model Accuracy Comparison per Fold (Walk-Forward Backtest)\", fontsize=16, fontweight='bold')\n    plt.xticks(index, results['fold_labels'], rotation=0, ha=\"center\")\n    if results['rf_acc_folds'] and results['log_acc_folds']:\n        y_max = max(max(results['rf_acc_folds']), max(results['log_acc_folds'])) * 1.1\n    else:\n        y_max = 1.0\n    plt.ylim(0.45, y_max)\n    plt.legend()\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No results to visualize. This might happen if the dataset is too small for the backtest splits.\")"
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-16T15:31:48.108861",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}